{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Zero error rate 2020-06-26.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slavamoiseev/Perl/blob/master/MNIST_Zero_error_rate_2020_06_26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn3eLJcNli27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchvision import datasets\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 20\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_data = datasets.MNIST(root='data', train=True,  download=True, transform=transform)\n",
        "test_data  = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
        "\n",
        "vector_labels = torch.tensor(\n",
        "  [\n",
        "    [0.0788, 0.1254, 0.0268, 0.2183, 0.3003, 0.3279, 0.0510, 0.0439, 0.3552,\n",
        "      0.0749, 0.0862, 0.3351, 0.0506, 0.1168, 0.1643, 0.1960, 0.2681, 0.2267,\n",
        "      0.1867, 0.3841, 0.0528, 0.1355, 0.0233, 0.0842, 0.2447], # digit 0\n",
        "    [0.0751, 0.1258, 0.2656, 0.0132, 0.0048, 0.0459, 0.0123, 0.0921, 0.0722,\n",
        "      0.4720, 0.0563, 0.0858, 0.1946, 0.0289, 0.0549, 0.0889, 0.0180, 0.0328,\n",
        "      0.2810, 0.0050, 0.2128, 0.0508, 0.6535, 0.0418, 0.2278],  # digit 1\n",
        "    [0.2016, 0.2286, 0.0246, 0.0350, 0.0284, 0.0113, 0.5795, 0.0695, 0.0989,\n",
        "      0.0515, 0.5213, 0.0253, 0.2721, 0.0178, 0.0095, 0.0061, 0.0243, 0.0309,\n",
        "      0.1076, 0.0285, 0.2499, 0.1712, 0.0729, 0.3033, 0.0366],  # digit 2\n",
        "    [0.0282, 0.1384, 0.0564, 0.0931, 0.0529, 0.0337, 0.1164, 0.1348, 0.0838,\n",
        "      0.0159, 0.3105, 0.0114, 0.1378, 0.0378, 0.0598, 0.7858, 0.0471, 0.1663,\n",
        "      0.0565, 0.0347, 0.0680, 0.2686, 0.0610, 0.2588, 0.0746],  # digit 3\n",
        "    [0.0368, 0.1348, 0.6532, 0.1854, 0.0323, 0.1109, 0.1187, 0.0222, 0.0124,\n",
        "      0.1473, 0.2992, 0.0402, 0.0864, 0.2666, 0.0860, 0.0477, 0.2516, 0.2643,\n",
        "      0.0793, 0.1082, 0.3297, 0.0828, 0.0344, 0.0016, 0.1499],  # digit 4\n",
        "    [0.4528, 0.1523, 0.0251, 0.0511, 0.1991, 0.0561, 0.1557, 0.6983, 0.0731,\n",
        "      0.1860, 0.0884, 0.0276, 0.1971, 0.1252, 0.1858, 0.0104, 0.0453, 0.1107,\n",
        "      0.0252, 0.0585, 0.0470, 0.1628, 0.1336, 0.0263, 0.1031],  # digit 5\n",
        "    [0.1442, 0.0414, 0.0260, 0.1051, 0.1011, 0.0484, 0.0940, 0.0715, 0.0459,\n",
        "      0.5043, 0.0312, 0.0336, 0.1456, 0.5837, 0.2772, 0.0289, 0.4086, 0.0950,\n",
        "      0.1037, 0.0274, 0.0142, 0.0024, 0.0353, 0.2247, 0.0419],  # digit 6\n",
        "    [0.0253, 0.0100, 0.0521, 0.1290, 0.1214, 0.1809, 0.0092, 0.0036, 0.0223,\n",
        "      0.0029, 0.0192, 0.0209, 0.5189, 0.1902, 0.2108, 0.0400, 0.0302, 0.1245,\n",
        "      0.4193, 0.0728, 0.0449, 0.5350, 0.2411, 0.1886, 0.0111],  # digit 7\n",
        "    [0.1489, 0.7321, 0.0011, 0.0065, 0.0215, 0.0462, 0.1617, 0.0958, 0.1073,\n",
        "      0.0546, 0.0956, 0.0459, 0.0222, 0.1168, 0.2183, 0.0008, 0.0317, 0.0330,\n",
        "      0.0749, 0.3990, 0.0605, 0.1267, 0.3394, 0.1195, 0.0142],  # digit 8\n",
        "    [0.5247, 0.0227, 0.2495, 0.0037, 0.1025, 0.2442, 0.0776, 0.0051, 0.0561,\n",
        "      0.0639, 0.0168, 0.1553, 0.1897, 0.0241, 0.0676, 0.1564, 0.0050, 0.0364,\n",
        "      0.0179, 0.0769, 0.0420, 0.0796, 0.2302, 0.5864, 0.2758]  # digit 9\n",
        "  ]\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlDmZFHbUFiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_data,  batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "\n",
        "train_targets = train_loader.dataset.targets\n",
        "test_targets  = test_loader.dataset.targets\n",
        "\n",
        "train_loader_digits = []\n",
        "\n",
        "rand_sample_size = 500\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "for i in range(vector_labels.size(0)):\n",
        "  ds = train_loader.dataset.data[train_targets == i].unsqueeze(1).float() / 255\n",
        "\n",
        "  train_loader_digits.append(\n",
        "    torch.utils.data.DataLoader(\n",
        "      torch.utils.data.TensorDataset(\n",
        "        ds[torch.randperm(ds.size(0))][0:rand_sample_size],\n",
        "        train_targets[train_targets == i][0:rand_sample_size]\n",
        "      ),\n",
        "      batch_size  = batch_size,\n",
        "      num_workers = num_workers,\n",
        "      shuffle     = True\n",
        "    )\n",
        "  )"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw_2AEp7y3q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(x, y):\n",
        "  return (1 - nn.CosineSimilarity(eps=1e-6)(x, y)).sum() / x.size(0)\n",
        "\n",
        "class MNIST_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(MNIST_Classifier, self).__init__()\n",
        "      \n",
        "      self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "      self.conv2 = nn.Conv2d(16, 8, 3, padding=2)\n",
        "      self.conv3 = nn.Conv2d(8,  1, 3, padding=2)\n",
        "\n",
        "      self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      x = self.pool(F.relu(self.conv1(x)))\n",
        "      x = self.pool(F.relu(self.conv2(x)))\n",
        "      x = self.pool(F.relu(self.conv3(x)))\n",
        "\n",
        "      return x\n",
        "\n",
        "    def init_weights(self):\n",
        "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.conv3.weight)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBRNwVrncG3b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e355ee09-7dd9-401f-9cfb-4250abb56f26"
      },
      "source": [
        "import copy\n",
        "\n",
        "def train_model(model, train_loader_digit, opt, n_epochs=20):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    for data in train_loader_digit:\n",
        "      images, labels = data\n",
        "\n",
        "      opt.zero_grad()\n",
        "\n",
        "      out = model(images.to(device)).view(images.size(0), 25)\n",
        "              \n",
        "      loss = loss_fn(out, torch.tensor([vector_labels[i].tolist() for i in labels]).to(device)) * images.size(0)\n",
        "                  \n",
        "      loss.backward()\n",
        "\n",
        "      opt.step()\n",
        "\n",
        "      train_loss += (loss.item() * images.size(0))\n",
        "\n",
        "    train_loss = train_loss / train_loader_digit.dataset.tensors[0].size(0)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "digit_models = []\n",
        "\n",
        "vector_labels = vector_labels[torch.randperm(vector_labels.size(0))]\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "for i in range(10):\n",
        "  model  = MNIST_Classifier().to(device)\n",
        "\n",
        "  opt = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
        "\n",
        "  model.init_weights()\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  print('Traning the model for digit {} (traning set size {})'.format(i, train_loader_digits[i].dataset.tensors[0].size(0)))\n",
        "\n",
        "  train_model(model, train_loader_digits[i], opt, 20)\n",
        "\n",
        "  digit_models.append(copy.deepcopy(model))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traning the model for digit 0 (traning set size 500)\n",
            "Epoch: 1 \tTraining Loss: 2.685939\n",
            "Epoch: 2 \tTraining Loss: 1.311522\n",
            "Epoch: 3 \tTraining Loss: 0.991907\n",
            "Epoch: 4 \tTraining Loss: 0.666352\n",
            "Epoch: 5 \tTraining Loss: 0.479967\n",
            "Epoch: 6 \tTraining Loss: 0.381389\n",
            "Epoch: 7 \tTraining Loss: 0.354205\n",
            "Epoch: 8 \tTraining Loss: 0.322875\n",
            "Epoch: 9 \tTraining Loss: 0.300588\n",
            "Epoch: 10 \tTraining Loss: 0.293006\n",
            "Epoch: 11 \tTraining Loss: 0.274947\n",
            "Epoch: 12 \tTraining Loss: 0.268353\n",
            "Epoch: 13 \tTraining Loss: 0.269105\n",
            "Epoch: 14 \tTraining Loss: 0.262721\n",
            "Epoch: 15 \tTraining Loss: 0.279266\n",
            "Epoch: 16 \tTraining Loss: 0.260747\n",
            "Epoch: 17 \tTraining Loss: 0.265940\n",
            "Epoch: 18 \tTraining Loss: 0.251868\n",
            "Epoch: 19 \tTraining Loss: 0.261001\n",
            "Epoch: 20 \tTraining Loss: 0.262204\n",
            "Traning the model for digit 1 (traning set size 500)\n",
            "Epoch: 1 \tTraining Loss: 2.450749\n",
            "Epoch: 2 \tTraining Loss: 0.528055\n",
            "Epoch: 3 \tTraining Loss: 0.446419\n",
            "Epoch: 4 \tTraining Loss: 0.423559\n",
            "Epoch: 5 \tTraining Loss: 0.407485\n",
            "Epoch: 6 \tTraining Loss: 0.398488\n",
            "Epoch: 7 \tTraining Loss: 0.388293\n",
            "Epoch: 8 \tTraining Loss: 0.381526\n",
            "Epoch: 9 \tTraining Loss: 0.376446\n",
            "Epoch: 10 \tTraining Loss: 0.372290\n",
            "Epoch: 11 \tTraining Loss: 0.373720\n",
            "Epoch: 12 \tTraining Loss: 0.373850\n",
            "Epoch: 13 \tTraining Loss: 0.369476\n",
            "Epoch: 14 \tTraining Loss: 0.352972\n",
            "Epoch: 15 \tTraining Loss: 0.318253\n",
            "Epoch: 16 \tTraining Loss: 0.318135\n",
            "Epoch: 17 \tTraining Loss: 0.311889\n",
            "Epoch: 18 \tTraining Loss: 0.310519\n",
            "Epoch: 19 \tTraining Loss: 0.310968\n",
            "Epoch: 20 \tTraining Loss: 0.310386\n",
            "Traning the model for digit 2 (traning set size 500)\n",
            "Epoch: 1 \tTraining Loss: 4.111148\n",
            "Epoch: 2 \tTraining Loss: 2.064427\n",
            "Epoch: 3 \tTraining Loss: 1.600040\n",
            "Epoch: 4 \tTraining Loss: 1.259639\n",
            "Epoch: 5 \tTraining Loss: 1.100443\n",
            "Epoch: 6 \tTraining Loss: 1.028843\n",
            "Epoch: 7 \tTraining Loss: 0.988041\n",
            "Epoch: 8 \tTraining Loss: 0.926556\n",
            "Epoch: 9 \tTraining Loss: 0.883999\n",
            "Epoch: 10 \tTraining Loss: 0.867640\n",
            "Epoch: 11 \tTraining Loss: 0.875528\n",
            "Epoch: 12 \tTraining Loss: 0.866324\n",
            "Epoch: 13 \tTraining Loss: 0.826757\n",
            "Epoch: 14 \tTraining Loss: 0.821156\n",
            "Epoch: 15 \tTraining Loss: 0.801754\n",
            "Epoch: 16 \tTraining Loss: 0.801408\n",
            "Epoch: 17 \tTraining Loss: 0.809026\n",
            "Epoch: 18 \tTraining Loss: 0.811485\n",
            "Epoch: 19 \tTraining Loss: 0.773832\n",
            "Epoch: 20 \tTraining Loss: 0.770631\n",
            "Traning the model for digit 3 (traning set size 500)\n",
            "Epoch: 1 \tTraining Loss: 6.250271\n",
            "Epoch: 2 \tTraining Loss: 3.478336\n",
            "Epoch: 3 \tTraining Loss: 1.704084\n",
            "Epoch: 4 \tTraining Loss: 1.321202\n",
            "Epoch: 5 \tTraining Loss: 1.147301\n",
            "Epoch: 6 \tTraining Loss: 1.038030\n",
            "Epoch: 7 \tTraining Loss: 0.943644\n",
            "Epoch: 8 \tTraining Loss: 0.917035\n",
            "Epoch: 9 \tTraining Loss: 0.859339\n",
            "Epoch: 10 \tTraining Loss: 0.837678\n",
            "Epoch: 11 \tTraining Loss: 0.768179\n",
            "Epoch: 12 \tTraining Loss: 0.713577\n",
            "Epoch: 13 \tTraining Loss: 0.691909\n",
            "Epoch: 14 \tTraining Loss: 0.637510\n",
            "Epoch: 15 \tTraining Loss: 0.612877\n",
            "Epoch: 16 \tTraining Loss: 0.594469\n",
            "Epoch: 17 \tTraining Loss: 0.571251\n",
            "Epoch: 18 \tTraining Loss: 0.549516\n",
            "Epoch: 19 \tTraining Loss: 0.539810\n",
            "Epoch: 20 \tTraining Loss: 0.545573\n",
            "Traning the model for digit 4 (traning set size 500)\n",
            "Epoch: 1 \tTraining Loss: 5.979703\n",
            "Epoch: 2 \tTraining Loss: 3.250479\n",
            "Epoch: 3 \tTraining Loss: 1.550174\n",
            "Epoch: 4 \tTraining Loss: 1.160918\n",
            "Epoch: 5 \tTraining Loss: 0.978687\n",
            "Epoch: 6 \tTraining Loss: 0.884158\n",
            "Epoch: 7 \tTraining Loss: 0.814868\n",
            "Epoch: 8 \tTraining Loss: 0.810052\n",
            "Epoch: 9 \tTraining Loss: 0.766847\n",
            "Epoch: 10 \tTraining Loss: 0.717308\n",
            "Epoch: 11 \tTraining Loss: 0.700008\n",
            "Epoch: 12 \tTraining Loss: 0.660918\n",
            "Epoch: 13 \tTraining Loss: 0.661962\n",
            "Epoch: 14 \tTraining Loss: 0.627198\n",
            "Epoch: 15 \tTraining Loss: 0.610202\n",
            "Epoch: 16 \tTraining Loss: 0.584310\n",
            "Epoch: 17 \tTraining Loss: 0.589440\n",
            "Epoch: 18 \tTraining Loss: 0.592057\n",
            "Epoch: 19 \tTraining Loss: 0.606386\n",
            "Epoch: 20 \tTraining Loss: 0.542468\n",
            "Traning the model for digit 5 (traning set size 500)\n",
            "Epoch: 1 \tTraining Loss: 1.582589\n",
            "Epoch: 2 \tTraining Loss: 0.496383\n",
            "Epoch: 3 \tTraining Loss: 0.331122\n",
            "Epoch: 4 \tTraining Loss: 0.268452\n",
            "Epoch: 5 \tTraining Loss: 0.247075\n",
            "Epoch: 6 \tTraining Loss: 0.224286\n",
            "Epoch: 7 \tTraining Loss: 0.201897\n",
            "Epoch: 8 \tTraining Loss: 0.187973\n",
            "Epoch: 9 \tTraining Loss: 0.179114\n",
            "Epoch: 10 \tTraining Loss: 0.171676\n",
            "Epoch: 11 \tTraining Loss: 0.171753\n",
            "Epoch: 12 \tTraining Loss: 0.161064\n",
            "Epoch: 13 \tTraining Loss: 0.158216\n",
            "Epoch: 14 \tTraining Loss: 0.155386\n",
            "Epoch: 15 \tTraining Loss: 0.154210\n",
            "Epoch: 16 \tTraining Loss: 0.150990\n",
            "Epoch: 17 \tTraining Loss: 0.145413\n",
            "Epoch: 18 \tTraining Loss: 0.147266\n",
            "Epoch: 19 \tTraining Loss: 0.139870\n",
            "Epoch: 20 \tTraining Loss: 0.144558\n",
            "Traning the model for digit 6 (traning set size 500)\n",
            "Epoch: 1 \tTraining Loss: 4.982052\n",
            "Epoch: 2 \tTraining Loss: 2.533196\n",
            "Epoch: 3 \tTraining Loss: 1.912503\n",
            "Epoch: 4 \tTraining Loss: 1.628042\n",
            "Epoch: 5 \tTraining Loss: 1.418901\n",
            "Epoch: 6 \tTraining Loss: 1.302475\n",
            "Epoch: 7 \tTraining Loss: 1.184714\n",
            "Epoch: 8 \tTraining Loss: 1.073027\n",
            "Epoch: 9 \tTraining Loss: 1.017707\n",
            "Epoch: 10 \tTraining Loss: 1.010447\n",
            "Epoch: 11 \tTraining Loss: 0.988014\n",
            "Epoch: 12 \tTraining Loss: 0.972657\n",
            "Epoch: 13 \tTraining Loss: 0.941137\n",
            "Epoch: 14 \tTraining Loss: 0.928784\n",
            "Epoch: 15 \tTraining Loss: 0.929907\n",
            "Epoch: 16 \tTraining Loss: 0.919370\n",
            "Epoch: 17 \tTraining Loss: 0.902484\n",
            "Epoch: 18 \tTraining Loss: 0.905221\n",
            "Epoch: 19 \tTraining Loss: 0.894782\n",
            "Epoch: 20 \tTraining Loss: 0.895229\n",
            "Traning the model for digit 7 (traning set size 500)\n",
            "Epoch: 1 \tTraining Loss: 4.889492\n",
            "Epoch: 2 \tTraining Loss: 3.457467\n",
            "Epoch: 3 \tTraining Loss: 2.776209\n",
            "Epoch: 4 \tTraining Loss: 2.390275\n",
            "Epoch: 5 \tTraining Loss: 1.981425\n",
            "Epoch: 6 \tTraining Loss: 0.633740\n",
            "Epoch: 7 \tTraining Loss: 0.391704\n",
            "Epoch: 8 \tTraining Loss: 0.324965\n",
            "Epoch: 9 \tTraining Loss: 0.298390\n",
            "Epoch: 10 \tTraining Loss: 0.284620\n",
            "Epoch: 11 \tTraining Loss: 0.268882\n",
            "Epoch: 12 \tTraining Loss: 0.265040\n",
            "Epoch: 13 \tTraining Loss: 0.255976\n",
            "Epoch: 14 \tTraining Loss: 0.255523\n",
            "Epoch: 15 \tTraining Loss: 0.247180\n",
            "Epoch: 16 \tTraining Loss: 0.244203\n",
            "Epoch: 17 \tTraining Loss: 0.241999\n",
            "Epoch: 18 \tTraining Loss: 0.238853\n",
            "Epoch: 19 \tTraining Loss: 0.236945\n",
            "Epoch: 20 \tTraining Loss: 0.233493\n",
            "Traning the model for digit 8 (traning set size 500)\n",
            "Epoch: 1 \tTraining Loss: 3.984118\n",
            "Epoch: 2 \tTraining Loss: 1.938241\n",
            "Epoch: 3 \tTraining Loss: 1.294539\n",
            "Epoch: 4 \tTraining Loss: 1.067542\n",
            "Epoch: 5 \tTraining Loss: 0.905112\n",
            "Epoch: 6 \tTraining Loss: 0.761994\n",
            "Epoch: 7 \tTraining Loss: 0.694429\n",
            "Epoch: 8 \tTraining Loss: 0.661514\n",
            "Epoch: 9 \tTraining Loss: 0.608397\n",
            "Epoch: 10 \tTraining Loss: 0.572903\n",
            "Epoch: 11 \tTraining Loss: 0.541938\n",
            "Epoch: 12 \tTraining Loss: 0.516575\n",
            "Epoch: 13 \tTraining Loss: 0.499766\n",
            "Epoch: 14 \tTraining Loss: 0.475912\n",
            "Epoch: 15 \tTraining Loss: 0.455855\n",
            "Epoch: 16 \tTraining Loss: 0.453336\n",
            "Epoch: 17 \tTraining Loss: 0.425932\n",
            "Epoch: 18 \tTraining Loss: 0.394412\n",
            "Epoch: 19 \tTraining Loss: 0.379533\n",
            "Epoch: 20 \tTraining Loss: 0.380997\n",
            "Traning the model for digit 9 (traning set size 500)\n",
            "Epoch: 1 \tTraining Loss: 3.721227\n",
            "Epoch: 2 \tTraining Loss: 1.825783\n",
            "Epoch: 3 \tTraining Loss: 1.383875\n",
            "Epoch: 4 \tTraining Loss: 0.948239\n",
            "Epoch: 5 \tTraining Loss: 0.683010\n",
            "Epoch: 6 \tTraining Loss: 0.598652\n",
            "Epoch: 7 \tTraining Loss: 0.554897\n",
            "Epoch: 8 \tTraining Loss: 0.528290\n",
            "Epoch: 9 \tTraining Loss: 0.521028\n",
            "Epoch: 10 \tTraining Loss: 0.507806\n",
            "Epoch: 11 \tTraining Loss: 0.493345\n",
            "Epoch: 12 \tTraining Loss: 0.478084\n",
            "Epoch: 13 \tTraining Loss: 0.469048\n",
            "Epoch: 14 \tTraining Loss: 0.463246\n",
            "Epoch: 15 \tTraining Loss: 0.454214\n",
            "Epoch: 16 \tTraining Loss: 0.434002\n",
            "Epoch: 17 \tTraining Loss: 0.432818\n",
            "Epoch: 18 \tTraining Loss: 0.446738\n",
            "Epoch: 19 \tTraining Loss: 0.427538\n",
            "Epoch: 20 \tTraining Loss: 0.425122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmA28XPqKugu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "77dd334e-eeed-4754-89c6-9b844a4a5f06"
      },
      "source": [
        "def eval_model(dataset_loader):\n",
        "  miss_classified = 0\n",
        "\n",
        "  for data in dataset_loader:\n",
        "    images, labels = data\n",
        "    \n",
        "    loss = []\n",
        "\n",
        "    for j in range(len(digit_models)):\n",
        "      out = digit_models[j](images.to(device)).view(images.size(0), 25)\n",
        "\n",
        "      loss.append(nn.CosineSimilarity(eps=1e-6)(out, torch.tensor([vector_labels[i].tolist() for i in labels]).to(device)).tolist())\n",
        "\n",
        "    miss_classified += (torch.tensor(loss).t().argsort()[:,-1] != labels).int().sum()\n",
        "  \n",
        "  return miss_classified\n",
        "\n",
        "print('Test set: miss-classified {} in {} images'.format(eval_model(test_loader), test_loader.dataset.data.size(0)))\n",
        "\n",
        "print('Train set: miss-classified {} in {} images'.format(eval_model(train_loader), train_loader.dataset.data.size(0)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set: miss-classified 0 in 10000 images\n",
            "Train set: miss-classified 1 in 60000 images\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}